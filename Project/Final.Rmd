---
title: 'TSA FINAL REPORT:'
author: "Yuxiang_Ren" 
date: "Spring 2023"
output: 
  pdf_document:
    toc: True
    includes:
      in_header: header.tex
csl: apa-6th-edition.csl
bibliography: reference.bib
link-citations: true
editor_options: 
  chunk_output_type: console
---
```{r packages, include=FALSE}


```

## Abstract



## Introduction



## Method (Data Processing)

> **Data**

Three datasets used in the project were collected from EDI Data Portal, including:

1. North Temperate Lakes LTER: Phytoplankton - Madison Lakes Area 1995 – current;[@Phytoplankton_data]
2. North Temperate Lakes LTER: Physical Limnology of Primary Study Lakes 1981 – current;
3. North Temperate Lakes LTER: Chemical Limnology of Primary Study Lakes: Nutrients, pH and Carbon 1981 – current.

The three files respectively record the water body phytoplankton information, physical information and chemical information of multiple lakes in the Wisconsin range. We analyzed these data at the beginning stage to screen out suitable research subjects, including the target lake, and primary algae responsible for blooms. First, we chose Mendota Lake (ME) for this project, as it has more time measurement data compared to other lakes, which might be more conducive to time series analysis and obtaining more reliable results (Table 1). Second, to obtain information on dominant species that may cause water blooms, we accumulated the biomass of algae from different divisions and considered the algae with the highest total biomass to be the main contributor to water bloom outbreaks. It is worth noting that the original data records the biomass of specific algal species on the observation day. Therefore, to obtain division-level data, we summed the biomass of all species within the same division on the same day to obtain the biomass information for the division. The result shows that the dominant division is Cyanophyta, which is also consistent with other studies (Table 2)[@Mendota1][@Mendota2].


```{r Data1, echo=FALSE, message=FALSE, warning=FALSE}
phyto <- read.csv( file = "./Data/Phyto_raw.csv")
phyto$sampledate <- ymd(phyto$sampledate)
rawdata <- phyto
site_date_info <- rawdata %>%
  group_by(lakeid) %>%
  summarize(Observation_Count = n_distinct(sampledate)) %>%
  arrange(desc(Observation_Count))
kable(site_date_info, col.names = c("Site", "Observation Date Count"), caption = "Site Information")

```

```{r Data2, echo=FALSE}
biomass_Division <- rawdata %>% group_by(division) %>% 
  mutate(Count = n()) %>% 
summarize('Count' = first(Count),'Total_Biomass' = sum(biomass_conc),'Max_Biomass' = max(biomass_conc, na.rm = TRUE),'Min_Biomass' = min(biomass_conc, na.rm = TRUE),'Mean_Biomass' = mean(biomass_conc, na.rm = TRUE))  %>% arrange(desc(Total_Biomass))

kable(biomass_Division[c(1:5),], 
      col.names = c("Division","Count", "Total Biomass", "Max Biomass", "Min Biomass", "Mean Biomass"),
      caption = "Division level Total Biomass (\\textit{mg/L})")
```

After identifying the target lake and algal division, we cleaned and combined the three data tables. The following are the data cleaning steps:

a.	Integrate the phytoplankton data according to lakeid, sampledate, depth range, and division to obtain the biomass information of each division on the observation day. Then, filter out all data with a lake id of Mendota and a division of Cyanophyta.
b.	Filter out the physical and chemical information of Lake Mendota. Considering that the original data records information at different depths on the same observation day, we calculated the average of all environmental data at depths of 0-8m, which correspond to the depths mentioned in the algae information. It is worth noting that on some dates, the depth of the algae information is 0-2m, and in these cases, we used the average environmental data for 0-2m.
c.	Based on the sampling date and depth range, we combined these data together (Table 3).  

```{r data cleanning1,echo=FALSE}
#see more detailed at "Data_wrangling_final1.RMD
load("./Data/ME_rawME_dayI.RDATA")

Table_ME_rawME_dayI_10 <- ME_rawME_dayI[c(1:6),c(1,2,5,6,12,13,14)]
kable(Table_ME_rawME_dayI_10,
      caption = "rawdata")

```
d.	We averaged the data monthly and used the zoo function (na.approx, rule = 2) to fill in NA values. Due to this method is not suitable for filling in NA values at the beginning of data, data before  1996 were removed.
e. The final dataset includes dates (from 1996 to December 2020), temperature, total nitrogen, total phosphorus, and biomass (Table 4).

```{r data cleanning2, echo=FALSE}
biomass_env <- read.csv(file= "./Data/biomass_Env.csv")

kable(biomass_env[c(1:5),],
      caption = "Final Data")
```



## Result ()

```{r cars}
#Load/install required package here
library(lubridate)
library(ggplot2)
library(forecast)  
library(Kendall)
library(tseries)
library(outliers)
library(tidyverse)
library(smooth)

#New package for M9 to assist with tables
#install.packages("kableExtra")
library(kableExtra)
```

Original (1996-2020)


```{r pressure, echo=FALSE}

biomass <- read.csv("./Data/biomass_Env.csv")

biomass_data <- biomass

# Preparing the data - create date object

biomass_data_processed <-
  biomass_data %>%
  mutate( Month = ymd(date) ) %>% 
  arrange(Month)

# Transform to the data frame

biomass_data_frame <- data.frame(Month=biomass_data_processed$Month, 
                          Biomass=biomass_data_processed$Biomass)



# Transform to time series format

ts_biomass_data <- ts(
  biomass_data_frame[1:300,2],
  start=c(year(biomass_data_frame$Month[1]),month(biomass_data_frame$Month[1])),
  frequency=12) 

ts_biomass <- ts(
  biomass_data_frame[1:288,2],
  start=c(year(biomass_data_frame$Month[1]),month(biomass_data_frame$Month[1])),
  frequency=12)


last_obs <- ts_biomass_data[289:300]



# Plot the time series, ACF, and PACF



TS_Plot <- ggplot(biomass_data_frame, aes(x=Month, y=Biomass)) +
      geom_line()

plot(TS_Plot)


#ACF and PACF plots
par(mfrow=c(1,2))
ACF_Plot <- Acf(ts_biomass_data, lag = 40, plot = TRUE,main="")
PACF_Plot <- Pacf(ts_biomass_data, lag = 40, plot = TRUE,main="")

```



```{r}
# Model 1: Arithmetic mean
# The meanf() has no holdout option
MEAN_seas <- meanf(y = ts_biomass, h = 12)  
checkresiduals(MEAN_seas)
plot(MEAN_seas)


# Model 2: Seasonal naive
SNAIVE_seas <- snaive(ts_biomass, h=12, holdout=FALSE)
checkresiduals(SNAIVE_seas)
plot(SNAIVE_seas)


# Model 3: SARIMA

SARIMA_autofit <- auto.arima(ts_biomass)
checkresiduals(SARIMA_autofit)

#Generating forecasts
#remember auto.arima does not call the forecast() internally so we need one more step
SARIMA_for <- forecast(SARIMA_autofit,h=12)
plot(SARIMA_for)


# Model 4: SS Exponential smoothing
SSES_seas <- es(ts_biomass,model="ZZZ",h=12,holdout=FALSE)
plot(SSES_seas)
checkresiduals(SSES_seas)


# Model 5: SS with StructTS()

SS_seas <- StructTS(ts_biomass,
                    type="BSM",fixed=c(0,0.001,0.3,NA))   #this function has convergence issues
checkresiduals(SS_seas)

#Generating forecasts
# StructTS() does not call the forecast() internally so we need one more step
SS_for <- forecast(SS_seas,h=12)
plot(SS_for)



#Model 1: Arithmetic mean
MEAN_scores <- accuracy(MEAN_seas$mean,last_obs)  #store the performance metrics

#Model 2: Seasonal naive 
SNAIVE_scores <- accuracy(SNAIVE_seas$mean,last_obs)

# Model 3:  SARIMA 
SARIMA_scores <- accuracy(SARIMA_for$mean,last_obs)

# Model 4:  SSES
SSES_scores <- accuracy(SSES_seas$forecast,last_obs)

# Model 5:  BSM 
SS_scores <- accuracy(SS_for$mean,last_obs)


#create data frame
seas_scores <- as.data.frame(rbind(MEAN_scores, SNAIVE_scores, SARIMA_scores,SSES_scores,SS_scores))
row.names(seas_scores) <- c("MEAN", "SNAIVE","SARIMA","SSES","BSM")

#choose model with lowest RMSE
best_model_index <- which.min(seas_scores[,"RMSE"])
cat("The best model by RMSE is:", row.names(seas_scores[best_model_index,])) 


kbl(seas_scores, 
      caption = "Forecast Accuracy for Seasonal Data",
      digits = array(5,ncol(seas_scores))) %>%
  kable_styling(full_width = FALSE, position = "center") %>%
  #highlight model with lowest RMSE
  kable_styling(latex_options="striped", stripe_index = which.min(seas_scores[,"RMSE"]))



autoplot(ts_biomass_data) +
  autolayer(MEAN_seas, PI=FALSE, series="Mean") +
  autolayer(SNAIVE_seas, PI=FALSE, series="Naïve") +
  autolayer(SARIMA_for,PI=FALSE, series="SARIMA") +
  autolayer(SSES_seas$forecast, series="SSES") +
  autolayer(SS_for,PI=FALSE,series="BSM") + 
  xlab("Month") + ylab("Electricity Retail Price ($/kWh)") +
  guides(colour=guide_legend(title="Forecast"))



autoplot(ts_biomass_data) +

autolayer(SARIMA_for,PI=FALSE, series="SARIMA") +
  xlab("Month") + ylab("Total Biomass (g/m2)") +
  guides(colour=guide_legend(title="Forecast"))



# Forecast

SARIMA_autofit_new <- auto.arima(ts_biomass_data)
checkresiduals(SARIMA_autofit_new)

SARIMA_for_new <- forecast(SARIMA_autofit_new,h=12)
plot(SARIMA_for_new)



```


Use recent ten-year data to forcast

```{r}
# Change the time span

# Transform to time series format

ts_biomass_data <- ts(
  biomass_data_frame[169:300,2],
  start=c(year(biomass_data_frame$Month[181]),month(biomass_data_frame$Month[181])),
  frequency=12) 

ts_biomass <- ts(
  biomass_data_frame[169:288,2],
  start=c(year(biomass_data_frame$Month[181]),month(biomass_data_frame$Month[181])),
  frequency=12)


last_obs <- ts_biomass_data[289:300]



```

```{r}
# Model 1: Arithmetic mean
# The meanf() has no holdout option
MEAN_seas <- meanf(y = ts_biomass, h = 12)  
checkresiduals(MEAN_seas)
plot(MEAN_seas)


# Model 2: Seasonal naive
SNAIVE_seas <- snaive(ts_biomass, h=12, holdout=FALSE)
checkresiduals(SNAIVE_seas)
plot(SNAIVE_seas)


# Model 3: SARIMA

SARIMA_autofit <- auto.arima(ts_biomass)
checkresiduals(SARIMA_autofit)

#Generating forecasts
#remember auto.arima does not call the forecast() internally so we need one more step
SARIMA_for <- forecast(SARIMA_autofit,h=12)
plot(SARIMA_for)


# Model 4: SS Exponential smoothing
SSES_seas <- es(ts_biomass,model="ZZZ",h=12,holdout=FALSE)
plot(SSES_seas)
checkresiduals(SSES_seas)


# Model 5: SS with StructTS()

SS_seas <- StructTS(ts_biomass,
                    type="BSM",fixed=c(0,0.001,0.3,NA))   #this function has convergence issues
checkresiduals(SS_seas)

#Generating forecasts
# StructTS() does not call the forecast() internally so we need one more step
SS_for <- forecast(SS_seas,h=12)
plot(SS_for)



#Model 1: Arithmetic mean
MEAN_scores <- accuracy(MEAN_seas$mean,last_obs)  #store the performance metrics

#Model 2: Seasonal naive 
SNAIVE_scores <- accuracy(SNAIVE_seas$mean,last_obs)

# Model 3:  SARIMA 
SARIMA_scores <- accuracy(SARIMA_for$mean,last_obs)

# Model 4:  SSES
SSES_scores <- accuracy(SSES_seas$forecast,last_obs)

# Model 5:  BSM 
SS_scores <- accuracy(SS_for$mean,last_obs)


#create data frame
seas_scores <- as.data.frame(rbind(MEAN_scores, SNAIVE_scores, SARIMA_scores,SSES_scores,SS_scores))
row.names(seas_scores) <- c("MEAN", "SNAIVE","SARIMA","SSES","BSM")

#choose model with lowest RMSE
best_model_index <- which.min(seas_scores[,"RMSE"])
cat("The best model by RMSE is:", row.names(seas_scores[best_model_index,])) 


kbl(seas_scores, 
      caption = "Forecast Accuracy for Seasonal Data",
      digits = array(5,ncol(seas_scores))) %>%
  kable_styling(full_width = FALSE, position = "center") %>%
  #highlight model with lowest RMSE
  kable_styling(latex_options="striped", stripe_index = which.min(seas_scores[,"RMSE"]))



autoplot(ts_biomass_data) +
  autolayer(MEAN_seas, PI=FALSE, series="Mean") +
  autolayer(SNAIVE_seas, PI=FALSE, series="Naïve") +
  autolayer(SARIMA_for,PI=FALSE, series="SARIMA") +
  autolayer(SSES_seas$forecast, series="SSES") +
  autolayer(SS_for,PI=FALSE,series="BSM") + 
  xlab("Month") + ylab("Electricity Retail Price ($/kWh)") +
  guides(colour=guide_legend(title="Forecast"))



autoplot(ts_biomass_data) +

autolayer(SARIMA_for,PI=FALSE, series="SARIMA") +
  xlab("Month") + ylab("Total Biomass (g/m2)") +
  guides(colour=guide_legend(title="Forecast"))



# Forecast

SARIMA_autofit_new <- auto.arima(ts_biomass_data)
checkresiduals(SARIMA_autofit_new)

SARIMA_for_new <- forecast(SARIMA_autofit_new,h=12)
plot(SARIMA_for_new)



```

```



## Discussion



## Reference
```{r}
#To add reference, you need first add reference in reference in reference.bib. 
#Than use"[@uniqueID]" to site it.

```



